{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"ca213fe1","cell_type":"markdown","source":"# ğŸ¥ Medical Data Ingestion & Embedding Pipeline for Kaggle GPU\n## Large-Scale ChromaDB Vector Store Creation\n\n**Purpose**: This notebook performs comprehensive medical data ingestion and embedding for a RAG (Retrieval-Augmented Generation) system. It processes two major medical datasets and creates persistent ChromaDB vector stores optimized for medical consultations.\n\n**Datasets Processed**:\n1. **EPFL Guidelines Dataset** - Clinical guidelines and treatment protocols\n2. **MedRAG/MedQuAD Dataset** - Medical question-answer pairs\n\n**Key Features**:\n- âš¡ **GPU-Optimized** - Leverages Kaggle's GPU environment for fast embedding generation\n- ğŸ—„ï¸ **Persistent Storage** - Creates ChromaDB collections that persist across sessions\n- ğŸ”§ **Memory Efficient** - Optimized memory management for large-scale processing\n- ğŸ“¦ **Downloadable Output** - Compressed archive ready for deployment\n\n**Output**: Two separate ChromaDB collections in a downloadable archive suitable for medical RAG systems.","metadata":{}},{"id":"787d084f","cell_type":"markdown","source":"## ğŸ“¦ Section 1: Environment Setup and Dependencies\n\nInstalling all required packages for the medical data ingestion pipeline. This cell installs LangChain ecosystem packages, ChromaDB for vector storage, HuggingFace transformers for embeddings, and Datasets library for efficient data loading.","metadata":{}},{"id":"8d5bcc0d","cell_type":"code","source":"!pip install langchain langchain_community langchain_huggingface chromadb sentence-transformers datasets\n\nprint(\"âœ… Package installation completed successfully!\")\nprint(\"ğŸ”§ Environment ready for medical data ingestion pipeline\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T12:40:46.598605Z","iopub.execute_input":"2025-07-23T12:40:46.598764Z","iopub.status.idle":"2025-07-23T12:43:02.314594Z","shell.execute_reply.started":"2025-07-23T12:40:46.598748Z","shell.execute_reply":"2025-07-23T12:43:02.313740Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\nCollecting langchain_community\n  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\nCollecting langchain_huggingface\n  Downloading langchain_huggingface-0.3.1-py3-none-any.whl.metadata (996 bytes)\nCollecting chromadb\n  Downloading chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.66)\nRequirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\nRequirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.1)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.4)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.12.13)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (8.5.0)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\nCollecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (1.26.4)\nCollecting langchain-core<1.0.0,>=0.3.66 (from langchain)\n  Downloading langchain_core-0.3.71-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.21.2)\nCollecting huggingface-hub>=0.33.4 (from langchain_huggingface)\n  Downloading huggingface_hub-0.33.4-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\nCollecting pybase64>=1.4.1 (from chromadb)\n  Downloading pybase64-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\nRequirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.3)\nCollecting posthog<6.0.0,>=2.4.0 (from chromadb)\n  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.14.0)\nCollecting onnxruntime>=1.14.1 (from chromadb)\n  Downloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.6 kB)\nCollecting opentelemetry-api>=1.2.0 (from chromadb)\n  Downloading opentelemetry_api-1.35.0-py3-none-any.whl.metadata (1.5 kB)\nCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n  Downloading opentelemetry_exporter_otlp_proto_grpc-1.35.0-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-sdk>=1.2.0 (from chromadb)\n  Downloading opentelemetry_sdk-1.35.0-py3-none-any.whl.metadata (1.5 kB)\nCollecting pypika>=0.48.9 (from chromadb)\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\nRequirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\nRequirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\nRequirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.73.1)\nCollecting bcrypt>=4.0.1 (from chromadb)\n  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\nCollecting kubernetes>=28.1.0 (from chromadb)\n  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\nCollecting mmh3>=4.0.1 (from chromadb)\n  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\nRequirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.18)\nRequirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (14.0.0)\nRequirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.24.0)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.52.4)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\nRequirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.6.15)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain_huggingface) (1.1.5)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.25.1)\nRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\nRequirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.40.3)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\nRequirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\nRequirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\nRequirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\nCollecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (2.4.1)\nCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\nRequirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\nRequirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\nCollecting opentelemetry-exporter-otlp-proto-common==1.35.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n  Downloading opentelemetry_exporter_otlp_proto_common-1.35.0-py3-none-any.whl.metadata (1.8 kB)\nCollecting opentelemetry-proto==1.35.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n  Downloading opentelemetry_proto-1.35.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting protobuf (from onnxruntime>=1.14.1->chromadb)\n  Downloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\nCollecting opentelemetry-semantic-conventions==0.56b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n  Downloading opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl.metadata (2.4 kB)\nCollecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\nCollecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\nCollecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nCollecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain_community) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain_community) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.2->langchain_community) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.26.2->langchain_community) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.26.2->langchain_community) (2024.2.0)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\nDownloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_huggingface-0.3.1-py3-none-any.whl (27 kB)\nDownloading chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\nDownloading huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m515.3/515.3 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.3.71-py3-none-any.whl (442 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m442.8/442.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading opentelemetry_api-1.35.0-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.35.0-py3-none-any.whl (18 kB)\nDownloading opentelemetry_exporter_otlp_proto_common-1.35.0-py3-none-any.whl (18 kB)\nDownloading opentelemetry_proto-1.35.0-py3-none-any.whl (72 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_sdk-1.35.0-py3-none-any.whl (119 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl (201 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pybase64-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (71 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\nDownloading durationpy-0.10-py3-none-any.whl (3.9 kB)\nDownloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl (321 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m321.1/321.1 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\nDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pypika\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=6b6a6f4893409528321b344bca84901b74234bf2ee4eac8d6e25e701d80f71a5\n  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\nSuccessfully built pypika\nInstalling collected packages: pypika, durationpy, uvloop, python-dotenv, pybase64, protobuf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mmh3, humanfriendly, httpx-sse, httptools, fsspec, bcrypt, backoff, watchfiles, posthog, opentelemetry-proto, opentelemetry-api, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, coloredlogs, pydantic-settings, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, nvidia-cusolver-cu12, kubernetes, opentelemetry-sdk, langchain-core, opentelemetry-exporter-otlp-proto-grpc, langchain_huggingface, onnxruntime, langchain_community, chromadb\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.5.1\n    Uninstalling fsspec-2025.5.1:\n      Successfully uninstalled fsspec-2025.5.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.33.1\n    Uninstalling huggingface-hub-0.33.1:\n      Successfully uninstalled huggingface-hub-0.33.1\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.3.66\n    Uninstalling langchain-core-0.3.66:\n      Successfully uninstalled langchain-core-0.3.66\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 6.31.1 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 6.31.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.31.1 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\ndataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.31.1 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.15 coloredlogs-15.0.1 durationpy-0.10 fsspec-2025.3.0 httptools-0.6.4 httpx-sse-0.4.1 huggingface-hub-0.33.4 humanfriendly-10.0 kubernetes-33.1.0 langchain-core-0.3.71 langchain_community-0.3.27 langchain_huggingface-0.3.1 mmh3-5.1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 onnxruntime-1.22.1 opentelemetry-api-1.35.0 opentelemetry-exporter-otlp-proto-common-1.35.0 opentelemetry-exporter-otlp-proto-grpc-1.35.0 opentelemetry-proto-1.35.0 opentelemetry-sdk-1.35.0 opentelemetry-semantic-conventions-0.56b0 posthog-5.4.0 protobuf-6.31.1 pybase64-1.4.1 pydantic-settings-2.10.1 pypika-0.48.9 python-dotenv-1.1.1 uvloop-0.21.0 watchfiles-1.1.0\nâœ… Package installation completed successfully!\nğŸ”§ Environment ready for medical data ingestion pipeline\n","output_type":"stream"}],"execution_count":1},{"id":"a0aae35a","cell_type":"markdown","source":"## âš™ï¸ Section 2: Configuration and Constants\n\nDefining global constants for consistent configuration across the entire data ingestion pipeline. These constants ensure proper file paths, collection naming, and embedding model configuration.","metadata":{}},{"id":"1a717a17","cell_type":"code","source":"import os\nimport sys\nfrom typing import List, Dict, Any\nimport gc\nimport time\n\n# =============================================================================\n# GLOBAL CONFIGURATION CONSTANTS - CRITICAL FOR CONSISTENCY\n# =============================================================================\n\n# Database and Storage Configuration\nDB_PERSIST_DIRECTORY = \"/kaggle/working/chroma_db\"\nGUIDELINES_COLLECTION_NAME = \"medical_guidelines\"\n\n# =========================================================================\n# --- Use the Textbooks dataset for the general collection ---\n# =========================================================================\nGENERAL_KNOWLEDGE_COLLECTION_NAME = \"medical_textbooks\" # More descriptive name\n\n# Embedding Model Configuration\nEMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n\n# Text Processing Configuration\nCHUNK_SIZE = 1000\nCHUNK_OVERLAP = 150\n\n# Dataset Configuration\nGUIDELINES_DATASET = \"epfl-llm/guidelines\"\n# =========================================================================\n# --- Point to the new Textbooks dataset ---\n# =========================================================================\nTEXTBOOKS_DATASET = \"MedRAG/textbooks\"\n\nprint(\"ğŸ”§ MEDICAL DATA INGESTION CONFIGURATION\")\nprint(\"=\" * 50)\nprint(f\"ğŸ“ Database Directory: {DB_PERSIST_DIRECTORY}\")\nprint(f\"ğŸ¥ Guidelines Collection: {GUIDELINES_COLLECTION_NAME}\")\nprint(f\"ğŸ“š Textbooks Collection: {GENERAL_KNOWLEDGE_COLLECTION_NAME}\")\nprint(f\"ğŸ¤– Embedding Model: {EMBEDDING_MODEL_NAME}\")\nprint(f\"ğŸ“„ Chunk Size: {CHUNK_SIZE} (Overlap: {CHUNK_OVERLAP})\")\nprint(f\"ğŸ“Š Guidelines Dataset: {GUIDELINES_DATASET}\")\nprint(f\"ğŸ“Š Textbooks Dataset: {TEXTBOOKS_DATASET}\")\nprint(\"=\" * 50)\nprint(\"âœ… Configuration loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T12:59:48.828521Z","iopub.execute_input":"2025-07-23T12:59:48.829139Z","iopub.status.idle":"2025-07-23T12:59:48.836677Z","shell.execute_reply.started":"2025-07-23T12:59:48.829116Z","shell.execute_reply":"2025-07-23T12:59:48.835896Z"}},"outputs":[{"name":"stdout","text":"ğŸ”§ MEDICAL DATA INGESTION CONFIGURATION\n==================================================\nğŸ“ Database Directory: /kaggle/working/chroma_db\nğŸ¥ Guidelines Collection: medical_guidelines\nğŸ“š Textbooks Collection: medical_textbooks\nğŸ¤– Embedding Model: sentence-transformers/all-MiniLM-L6-v2\nğŸ“„ Chunk Size: 1000 (Overlap: 150)\nğŸ“Š Guidelines Dataset: epfl-llm/guidelines\nğŸ“Š Textbooks Dataset: MedRAG/textbooks\n==================================================\nâœ… Configuration loaded successfully!\n","output_type":"stream"}],"execution_count":7},{"id":"95718784","cell_type":"markdown","source":"## ğŸ¤– Section 3: Embedding Model Initialization\n\nInitializing the HuggingFace embedding model with GPU optimization. The model will automatically detect and use available GPU resources in the Kaggle environment for maximum performance.","metadata":{}},{"id":"311e1e3b","cell_type":"code","source":"from langchain_huggingface import HuggingFaceEmbeddings\nimport torch\n\n# =============================================================================\n# EMBEDDING MODEL INITIALIZATION WITH GPU OPTIMIZATION\n# =============================================================================\n\nprint(\"ğŸ¤– INITIALIZING EMBEDDING MODEL FOR MEDICAL DATA PROCESSING\")\nprint(\"=\" * 60)\n\n# Check GPU availability\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"ğŸ”§ Computing Device: {device}\")\n\nif device == \"cuda\":\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n    print(f\"âš¡ GPU: {gpu_name}\")\n    print(f\"ğŸ’¾ GPU Memory: {gpu_memory:.1f} GB\")\nelse:\n    print(\"âš ï¸  Warning: GPU not available, using CPU (slower processing)\")\n\nprint(f\"ğŸ”„ Loading embedding model: {EMBEDDING_MODEL_NAME}\")\nprint(\"   Note: This model will automatically leverage GPU acceleration\")\n\n# =========================================================================\n# --- Initialize the all-MiniLM-L6-v2 model ---\n# This model does NOT require the 'trust_remote_code' parameter.\n# =========================================================================\nembedding_model = HuggingFaceEmbeddings(\n    model_name=EMBEDDING_MODEL_NAME,\n    model_kwargs={\n        'device': device            # Use GPU if available\n    },\n    encode_kwargs={\n        'normalize_embeddings': True  # Normalizing embeddings is good practice for this model\n    }\n)\n# --- END OF MODIFICATION ---\n\nprint(\"âœ… Embedding model initialized successfully!\")\nprint(f\"ğŸ¯ Model ready for medical document embedding on {device.upper()}\")\nprint(\"=\" * 60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T12:43:51.692010Z","iopub.execute_input":"2025-07-23T12:43:51.692272Z","iopub.status.idle":"2025-07-23T12:44:35.290825Z","shell.execute_reply.started":"2025-07-23T12:43:51.692250Z","shell.execute_reply":"2025-07-23T12:44:35.289982Z"}},"outputs":[{"name":"stdout","text":"ğŸ¤– INITIALIZING EMBEDDING MODEL FOR MEDICAL DATA PROCESSING\n============================================================\nğŸ”§ Computing Device: cuda\nâš¡ GPU: Tesla T4\nğŸ’¾ GPU Memory: 14.7 GB\nğŸ”„ Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n   Note: This model will automatically leverage GPU acceleration\n","output_type":"stream"},{"name":"stderr","text":"2025-07-23 12:44:09.549857: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753274649.871975      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753274649.960836      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31ce65afe96b497090ad3dda116be6d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2098c1251971485fb9f1815a727d3c32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cffce2063794adf944b1c85f8accf7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f340f9edf7e446bb0ca89e6632359ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfabb940097541249dbfee8b1ffa1745"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b92542441df54169ae8fcd304c4960bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59b30b15f0a048cc8c4e13b2e0ad5c42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85424ef02f7849f0871d8dc8f32ac032"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ef2f37991c54468a080ec3e0643e451"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d571de5f6024074b9d62971d2f37a7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bee9b19431c04186b9a52f6027d94a18"}},"metadata":{}},{"name":"stdout","text":"âœ… Embedding model initialized successfully!\nğŸ¯ Model ready for medical document embedding on CUDA\n============================================================\n","output_type":"stream"}],"execution_count":4},{"id":"524980f3","cell_type":"markdown","source":"## ğŸ¥ Section 4: Guidelines Dataset Processing\n\nProcessing the EPFL Guidelines dataset containing clinical guidelines and treatment protocols. This section loads the dataset, performs text chunking, and ingests the processed documents into a dedicated ChromaDB collection.","metadata":{}},{"id":"1f5735d1","cell_type":"code","source":"from datasets import load_dataset\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.vectorstores import Chroma\nfrom langchain.docstore.document import Document\n\n# =============================================================================\n# PART A: GUIDELINES DATASET PROCESSING\n# =============================================================================\n\nprint(\"ğŸ¥ PROCESSING MEDICAL GUIDELINES DATASET\")\nprint(\"=\" * 60)\n\n# Initialize text splitter for chunking documents\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=CHUNK_SIZE,\n    chunk_overlap=CHUNK_OVERLAP,\n    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n    length_function=len\n)\n\nprint(f\"ğŸ”§ Text Splitter configured:\")\nprint(f\"   ğŸ“ Chunk Size: {CHUNK_SIZE} characters\")\nprint(f\"   ğŸ”„ Overlap: {CHUNK_OVERLAP} characters\")\n\n# Load the EPFL Guidelines dataset\nprint(f\"ğŸ“Š Loading dataset: {GUIDELINES_DATASET}\")\nstart_time = time.time()\n\ntry:\n    guidelines_dataset = load_dataset(GUIDELINES_DATASET, split=\"train\")\n    load_time = time.time() - start_time\n    \n    print(f\"âœ… Full dataset loaded successfully in {load_time:.2f} seconds\")\n    print(f\"ğŸ“‹ Full dataset size: {len(guidelines_dataset)} documents\")\n    \n    # FOR DEMONSTRATION: Sample 10,000 documents\n    print(f\"\\nğŸ”¥ Taking a random sample of 10,000 documents for demonstration purposes...\")\n    if len(guidelines_dataset) > 10000:\n        guidelines_dataset = guidelines_dataset.shuffle(seed=42).select(range(10000))\n        print(f\"âœ… Sampled dataset size: {len(guidelines_dataset)} documents\")\n    else:\n        print(\"âš ï¸  Dataset has fewer than 10,000 documents, using all of them.\")\n    \n    if len(guidelines_dataset) > 0:\n        sample = guidelines_dataset[0]\n        print(f\"ğŸ“„ Sample fields: {list(sample.keys())}\")\n        if 'clean_text' in sample:\n            print(f\"ğŸ“ Sample text length: {len(sample['clean_text'])} characters\")\n    \nexcept Exception as e:\n    print(f\"âŒ Error loading or sampling dataset: {e}\")\n    raise\n\n# Process and chunk the guidelines documents\nprint(f\"\\nğŸ”„ Processing and chunking {len(guidelines_dataset)} guidelines documents...\")\ndocuments = []\n\nfor i, item in enumerate(guidelines_dataset):\n    if i % 1000 == 0:\n        print(f\"   Processing document {i+1}/{len(guidelines_dataset)}\")\n    \n    if 'clean_text' in item and item['clean_text']:\n        doc = Document(\n            page_content=item['clean_text'],\n            metadata={'source': 'epfl_guidelines', 'doc_id': i, 'dataset': GUIDELINES_DATASET}\n        )\n        documents.append(doc)\n\nprint(f\"âœ… Created {len(documents)} documents from guidelines dataset\")\n\n# Chunk all documents\nprint(f\"ğŸ”„ Chunking documents...\")\nchunked_documents = text_splitter.split_documents(documents)\n\nprint(f\"âœ… Created {len(chunked_documents)} chunks from {len(documents)} documents\")\nprint(f\"ğŸ“Š Average chunks per document: {len(chunked_documents) / len(documents):.1f}\")\n\n# Ingest into ChromaDB\nprint(f\"\\nğŸ’¾ Ingesting chunks into ChromaDB...\")\nprint(f\"   ğŸ—„ï¸  Collection: {GUIDELINES_COLLECTION_NAME}\")\nprint(f\"   ğŸ“ Persist Directory: {DB_PERSIST_DIRECTORY}\")\n\nos.makedirs(DB_PERSIST_DIRECTORY, exist_ok=True)\n\ntry:\n    guidelines_db = Chroma.from_documents(\n        documents=chunked_documents,\n        embedding=embedding_model,\n        persist_directory=DB_PERSIST_DIRECTORY,\n        collection_name=GUIDELINES_COLLECTION_NAME\n    )\n    \n    collection_count = guidelines_db._collection.count()\n    \n    print(f\"âœ… GUIDELINES DATASET INGESTION COMPLETED SUCCESSFULLY!\")\n    print(f\"ğŸ“Š Total chunks ingested into '{GUIDELINES_COLLECTION_NAME}': {collection_count}\")\n    print(f\"ğŸ’¾ Database persisted to: {DB_PERSIST_DIRECTORY}\")\n    \nexcept Exception as e:\n    print(f\"âŒ Error during ChromaDB ingestion: {e}\")\n    raise\n\n# Clear memory\nguidelines_db = None\ndocuments = None\nchunked_documents = None\nguidelines_dataset = None\ngc.collect()\n\nprint(f\"ğŸ§¹ Memory cleared for next dataset processing\")\nprint(\"=\" * 60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T12:44:47.059092Z","iopub.execute_input":"2025-07-23T12:44:47.059624Z","iopub.status.idle":"2025-07-23T12:57:05.346248Z","shell.execute_reply.started":"2025-07-23T12:44:47.059600Z","shell.execute_reply":"2025-07-23T12:57:05.345379Z"}},"outputs":[{"name":"stdout","text":"ğŸ¥ PROCESSING MEDICAL GUIDELINES DATASET\n============================================================\nğŸ”§ Text Splitter configured:\n   ğŸ“ Chunk Size: 1000 characters\n   ğŸ”„ Overlap: 150 characters\nğŸ“Š Loading dataset: epfl-llm/guidelines\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"933f4625eddc42e4b40c41e8332859f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"open_guidelines.jsonl:   0%|          | 0.00/878M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00169e4f0b2440778fb54ed0123c6bf9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"592508dea72e444182dca61388f0a9ec"}},"metadata":{}},{"name":"stdout","text":"âœ… Full dataset loaded successfully in 7.94 seconds\nğŸ“‹ Full dataset size: 37970 documents\n\nğŸ”¥ Taking a random sample of 10,000 documents for demonstration purposes...\nâœ… Sampled dataset size: 10000 documents\nğŸ“„ Sample fields: ['id', 'source', 'title', 'clean_text', 'raw_text', 'url', 'overview']\nğŸ“ Sample text length: 42213 characters\n\nğŸ”„ Processing and chunking 10000 guidelines documents...\n   Processing document 1/10000\n   Processing document 1001/10000\n   Processing document 2001/10000\n   Processing document 3001/10000\n   Processing document 4001/10000\n   Processing document 5001/10000\n   Processing document 6001/10000\n   Processing document 7001/10000\n   Processing document 8001/10000\n   Processing document 9001/10000\nâœ… Created 10000 documents from guidelines dataset\nğŸ”„ Chunking documents...\nâœ… Created 188808 chunks from 10000 documents\nğŸ“Š Average chunks per document: 18.9\n\nğŸ’¾ Ingesting chunks into ChromaDB...\n   ğŸ—„ï¸  Collection: medical_guidelines\n   ğŸ“ Persist Directory: /kaggle/working/chroma_db\nâœ… GUIDELINES DATASET INGESTION COMPLETED SUCCESSFULLY!\nğŸ“Š Total chunks ingested into 'medical_guidelines': 188808\nğŸ’¾ Database persisted to: /kaggle/working/chroma_db\nğŸ§¹ Memory cleared for next dataset processing\n============================================================\n","output_type":"stream"}],"execution_count":5},{"id":"f2ec506d","cell_type":"markdown","source":"## â“ Section 5: MedQuAD Dataset Processing\n\nProcessing the MedRAG/MedQuAD dataset containing medical question-answer pairs. This section formats the Q&A pairs into structured documents, applies text chunking, and ingests them into a separate ChromaDB collection for medical consultation queries.","metadata":{}},{"id":"aefeb299","cell_type":"code","source":"# =============================================================================\n# PART B: MEDICAL TEXTBOOKS DATASET PROCESSING\n# =============================================================================\n\nprint(\"ğŸ“š PROCESSING MEDICAL TEXTBOOKS DATASET\")\nprint(\"=\" * 60)\n\n# Load the MedRAG/textbooks dataset\nprint(f\"ğŸ“Š Loading dataset: {TEXTBOOKS_DATASET}\")\nstart_time = time.time()\n\ntry:\n    textbooks_dataset = load_dataset(TEXTBOOKS_DATASET, split=\"train\")\n    load_time = time.time() - start_time\n    \n    print(f\"âœ… Full dataset loaded successfully in {load_time:.2f} seconds\")\n    print(f\"ğŸ“‹ Full dataset size: {len(textbooks_dataset)} documents\")\n    \n    # FOR DEMONSTRATION: Sample 10,000 documents\n    print(f\"\\nğŸ”¥ Taking a random sample of 10,000 documents for demonstration purposes...\")\n    if len(textbooks_dataset) > 10000:\n        textbooks_dataset = textbooks_dataset.shuffle(seed=42).select(range(10000))\n        print(f\"âœ… Sampled dataset size: {len(textbooks_dataset)} documents\")\n    else:\n        print(\"âš ï¸  Dataset has fewer than 10,000 documents, using all of them.\")\n\n    # Show sample data structure for the textbooks dataset\n    if len(textbooks_dataset) > 0:\n        sample = textbooks_dataset[0]\n        print(f\"ğŸ“„ Sample fields: {list(sample.keys())}\")\n        # =========================================================================\n        # --- Use the 'content' column name for the check ---\n        # =========================================================================\n        if 'content' in sample:\n            print(f\"ğŸ“ Sample text length: {len(sample['content'])} characters\")\n        if 'title' in sample:\n            print(f\"ğŸ“– Sample from book (using title): {sample['title']}\")\n    \nexcept Exception as e:\n    print(f\"âŒ Error loading or sampling dataset: {e}\")\n    raise\n\n# Process and prepare documents from the textbook dataset\nprint(f\"\\nğŸ”„ Processing {len(textbooks_dataset)} textbook documents...\")\ntextbook_documents = []\n\nfor i, item in enumerate(textbooks_dataset):\n    if i % 1000 == 0:\n        print(f\"   Processing textbook document {i+1}/{len(textbooks_dataset)}\")\n    \n    # =========================================================================\n    # --- Use the 'content' column name for processing ---\n    # =========================================================================\n    if 'content' in item and item['content']:\n        doc = Document(\n            page_content=item['content'], # Use 'content' here\n            metadata={\n                'source': 'medrag_textbooks',\n                'title': item.get('title', 'Unknown'), # Use 'title' for book name\n                'chapter': item.get('contents', 'Unknown'), # Use 'contents' for chapter\n                'doc_id': item.get('id', i)\n            }\n        )\n        textbook_documents.append(doc)\n\nprint(f\"âœ… Created {len(textbook_documents)} documents from the textbooks dataset\")\n\n# Chunk the formatted textbook documents using the same text splitter\nprint(f\"ğŸ”„ Chunking textbook documents...\")\ntextbook_chunked_documents = text_splitter.split_documents(textbook_documents)\n\nprint(f\"âœ… Created {len(textbook_chunked_documents)} chunks from {len(textbook_documents)} documents\")\n\n# This will no longer cause a ZeroDivisionError\nif len(textbook_documents) > 0:\n    print(f\"ğŸ“Š Average chunks per textbook document: {len(textbook_chunked_documents) / len(textbook_documents):.1f}\")\n\n# Ingest into ChromaDB with the new collection name\nprint(f\"\\nğŸ’¾ Ingesting textbook chunks into ChromaDB...\")\nprint(f\"   ğŸ—„ï¸  Collection: {GENERAL_KNOWLEDGE_COLLECTION_NAME}\")\nprint(f\"   ğŸ“ Persist Directory: {DB_PERSIST_DIRECTORY}\")\n\ntry:\n    textbooks_db = Chroma.from_documents(\n        documents=textbook_chunked_documents,\n        embedding=embedding_model,\n        persist_directory=DB_PERSIST_DIRECTORY,\n        collection_name=GENERAL_KNOWLEDGE_COLLECTION_NAME\n    )\n    \n    textbooks_collection_count = textbooks_db._collection.count()\n    \n    print(f\"âœ… TEXTBOOKS DATASET INGESTION COMPLETED SUCCESSFULLY!\")\n    print(f\"ğŸ“Š Total chunks ingested into '{GENERAL_KNOWLEDGE_COLLECTION_NAME}': {textbooks_collection_count}\")\n    print(f\"ğŸ’¾ Database persisted to: {DB_PERSIST_DIRECTORY}\")\n    \nexcept Exception as e:\n    print(f\"âŒ Error during ChromaDB ingestion: {e}\")\n    raise\n\n# Clear memory after processing\ntextbooks_db = None\ntextbook_documents = None\ntextbook_chunked_documents = None\ntextbooks_dataset = None\ngc.collect()\n\nprint(f\"ğŸ§¹ Memory cleared after textbook dataset processing\")\nprint(\"=\" * 60)\n\n# Summary of both collections\nprint(f\"\\nğŸ“Š INGESTION SUMMARY:\")\nprint(f\"   ğŸ¥ Guidelines Collection: '{GUIDELINES_COLLECTION_NAME}'\")\nprint(f\"   ğŸ“š Textbooks Collection: '{GENERAL_KNOWLEDGE_COLLECTION_NAME}'\")\nprint(f\"   ğŸ“ Database Location: {DB_PERSIST_DIRECTORY}\")\nprint(\"âœ… Both medical datasets successfully ingested into separate ChromaDB collections!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T13:01:36.776058Z","iopub.execute_input":"2025-07-23T13:01:36.776714Z","iopub.status.idle":"2025-07-23T13:02:21.873100Z","shell.execute_reply.started":"2025-07-23T13:01:36.776687Z","shell.execute_reply":"2025-07-23T13:02:21.872303Z"}},"outputs":[{"name":"stdout","text":"ğŸ“š PROCESSING MEDICAL TEXTBOOKS DATASET\n============================================================\nğŸ“Š Loading dataset: MedRAG/textbooks\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dfc5e5bb9d74b5397618121208d89e4"}},"metadata":{}},{"name":"stdout","text":"âœ… Full dataset loaded successfully in 0.51 seconds\nğŸ“‹ Full dataset size: 125847 documents\n\nğŸ”¥ Taking a random sample of 10,000 documents for demonstration purposes...\nâœ… Sampled dataset size: 10000 documents\nğŸ“„ Sample fields: ['id', 'title', 'content', 'contents']\nğŸ“ Sample text length: 548 characters\nğŸ“– Sample from book (using title): Neurology_Adams\n\nğŸ”„ Processing 10000 textbook documents...\n   Processing textbook document 1/10000\n   Processing textbook document 1001/10000\n   Processing textbook document 2001/10000\n   Processing textbook document 3001/10000\n   Processing textbook document 4001/10000\n   Processing textbook document 5001/10000\n   Processing textbook document 6001/10000\n   Processing textbook document 7001/10000\n   Processing textbook document 8001/10000\n   Processing textbook document 9001/10000\nâœ… Created 10000 documents from the textbooks dataset\nğŸ”„ Chunking textbook documents...\nâœ… Created 10000 chunks from 10000 documents\nğŸ“Š Average chunks per textbook document: 1.0\n\nğŸ’¾ Ingesting textbook chunks into ChromaDB...\n   ğŸ—„ï¸  Collection: medical_textbooks\n   ğŸ“ Persist Directory: /kaggle/working/chroma_db\nâœ… TEXTBOOKS DATASET INGESTION COMPLETED SUCCESSFULLY!\nğŸ“Š Total chunks ingested into 'medical_textbooks': 10000\nğŸ’¾ Database persisted to: /kaggle/working/chroma_db\nğŸ§¹ Memory cleared after textbook dataset processing\n============================================================\n\nğŸ“Š INGESTION SUMMARY:\n   ğŸ¥ Guidelines Collection: 'medical_guidelines'\n   ğŸ“š Textbooks Collection: 'medical_textbooks'\n   ğŸ“ Database Location: /kaggle/working/chroma_db\nâœ… Both medical datasets successfully ingested into separate ChromaDB collections!\n","output_type":"stream"}],"execution_count":9},{"id":"5956a0e8","cell_type":"markdown","source":"## ğŸ“¦ Section 6: Final Verification and Archive Creation\n\nCreating a compressed archive of the complete ChromaDB database for easy download from Kaggle. This section also performs final verification to ensure both collections were created successfully and are ready for deployment.","metadata":{}},{"id":"759827d9","cell_type":"code","source":"# =============================================================================\n# FINAL VERIFICATION AND ARCHIVE CREATION\n# =============================================================================\n\nprint(\"ğŸ” FINAL VERIFICATION AND ARCHIVE CREATION\")\nprint(\"=\" * 60)\n\n# Verify that the database directory exists and contains expected files\nprint(f\"ğŸ“ Verifying database directory: {DB_PERSIST_DIRECTORY}\")\n\nif os.path.exists(DB_PERSIST_DIRECTORY):\n    print(\"âœ… Database directory exists\")\n    \n    # List contents of the database directory\n    db_contents = os.listdir(DB_PERSIST_DIRECTORY)\n    print(f\"ğŸ“‚ Database directory contents: {len(db_contents)} items\")\n    \n    for item in sorted(db_contents):\n        item_path = os.path.join(DB_PERSIST_DIRECTORY, item)\n        if os.path.isdir(item_path):\n            print(f\"   ğŸ“ {item}/ (directory)\")\n        else:\n            file_size = os.path.getsize(item_path) / 1024 / 1024  # MB\n            print(f\"   ğŸ“„ {item} ({file_size:.2f} MB)\")\n    \n    # Verify collections by attempting to connect to them\n    try:\n        print(f\"\\nğŸ” Verifying collections...\")\n        \n        # Test Guidelines collection\n        guidelines_test_db = Chroma(\n            persist_directory=DB_PERSIST_DIRECTORY,\n            embedding_function=embedding_model,\n            collection_name=GUIDELINES_COLLECTION_NAME\n        )\n        guidelines_count = guidelines_test_db._collection.count()\n        print(f\"âœ… Guidelines collection '{GUIDELINES_COLLECTION_NAME}': {guidelines_count} documents\")\n        \n        # Test Q&A collection\n        qna_test_db = Chroma(\n            persist_directory=DB_PERSIST_DIRECTORY,\n            embedding_function=embedding_model,\n            collection_name=QNA_COLLECTION_NAME\n        )\n        qna_count = qna_test_db._collection.count()\n        print(f\"âœ… Q&A collection '{QNA_COLLECTION_NAME}': {qna_count} documents\")\n        \n        total_documents = guidelines_count + qna_count\n        print(f\"ğŸ“Š Total documents across both collections: {total_documents}\")\n        \n        # Clean up test connections\n        guidelines_test_db = None\n        qna_test_db = None\n        \n    except Exception as e:\n        print(f\"âš ï¸  Warning: Could not verify collections: {e}\")\n        print(\"   Database files exist but verification failed\")\n    \nelse:\n    print(f\"âŒ Database directory does not exist: {DB_PERSIST_DIRECTORY}\")\n    raise FileNotFoundError(f\"Database directory not found: {DB_PERSIST_DIRECTORY}\")\n\n# Create compressed archive for download\nprint(f\"\\nğŸ“¦ Creating compressed archive for download...\")\narchive_name = \"chroma_db.zip\"\n\ntry:\n    # Use the zip command to create a compressed archive\n    zip_command = f\"cd /kaggle/working && zip -r {archive_name} chroma_db\"\n    exit_code = os.system(zip_command)\n    \n    if exit_code == 0:\n        archive_path = f\"/kaggle/working/{archive_name}\"\n        if os.path.exists(archive_path):\n            archive_size = os.path.getsize(archive_path) / 1024 / 1024  # MB\n            print(f\"âœ… Archive created successfully: {archive_name}\")\n            print(f\"ğŸ“¦ Archive size: {archive_size:.2f} MB\")\n            print(f\"ğŸ“ Archive location: {archive_path}\")\n        else:\n            print(f\"âŒ Archive file not found after creation\")\n    else:\n        print(f\"âŒ Zip command failed with exit code: {exit_code}\")\n        \n    # Alternative method using Python's zipfile if the command fails\n    if not os.path.exists(f\"/kaggle/working/{archive_name}\"):\n        print(\"ğŸ”„ Attempting alternative zip creation method...\")\n        import zipfile\n        import shutil\n        \n        with zipfile.ZipFile(f\"/kaggle/working/{archive_name}\", 'w', zipfile.ZIP_DEFLATED) as zipf:\n            for root, dirs, files in os.walk(DB_PERSIST_DIRECTORY):\n                for file in files:\n                    file_path = os.path.join(root, file)\n                    arc_path = os.path.relpath(file_path, os.path.dirname(DB_PERSIST_DIRECTORY))\n                    zipf.write(file_path, arc_path)\n        \n        if os.path.exists(f\"/kaggle/working/{archive_name}\"):\n            archive_size = os.path.getsize(f\"/kaggle/working/{archive_name}\") / 1024 / 1024\n            print(f\"âœ… Alternative archive creation successful: {archive_size:.2f} MB\")\n        else:\n            print(f\"âŒ Alternative archive creation failed\")\n    \nexcept Exception as e:\n    print(f\"âŒ Error creating archive: {e}\")\n\n# Final completion message\nprint(f\"\\nğŸ‰ MEDICAL DATA INGESTION PIPELINE COMPLETED SUCCESSFULLY!\")\nprint(\"=\" * 60)\nprint(f\"âœ… Two medical datasets processed and ingested:\")\nprint(f\"   ğŸ¥ {GUIDELINES_DATASET} â†’ '{GUIDELINES_COLLECTION_NAME}' collection\")\nprint(f\"   â“ {MEDQUAD_DATASET} â†’ '{QNA_COLLECTION_NAME}' collection\")\nprint(f\"ğŸ’¾ ChromaDB database location: {DB_PERSIST_DIRECTORY}\")\nprint(f\"ğŸ“¦ Downloadable archive: /kaggle/working/{archive_name}\")\nprint(f\"ğŸ¤– Embedding model: {EMBEDDING_MODEL_NAME}\")\nprint(f\"âš¡ Processed on: {device.upper()}\")\nprint(\"=\" * 60)\nprint(\"ğŸš€ The vector database is ready for deployment in medical RAG systems!\")\nprint(\"ğŸ“¥ Download the zip file to use in your medical consultation application.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T13:03:15.792056Z","iopub.execute_input":"2025-07-23T13:03:15.792339Z","iopub.status.idle":"2025-07-23T13:04:44.424057Z","shell.execute_reply.started":"2025-07-23T13:03:15.792318Z","shell.execute_reply":"2025-07-23T13:04:44.423238Z"}},"outputs":[{"name":"stdout","text":"ğŸ” FINAL VERIFICATION AND ARCHIVE CREATION\n============================================================\nğŸ“ Verifying database directory: /kaggle/working/chroma_db\nâœ… Database directory exists\nğŸ“‚ Database directory contents: 3 items\n   ğŸ“ 2f13116e-5472-4689-a051-f846525d265a/ (directory)\n   ğŸ“ 98b16966-98a1-4177-aa1a-7e56bfb1ac5d/ (directory)\n   ğŸ“„ chroma.sqlite3 (1011.16 MB)\n\nğŸ” Verifying collections...\nâœ… Guidelines collection 'medical_guidelines': 188808 documents\nâœ… Q&A collection 'medrag_qna': 0 documents\nğŸ“Š Total documents across both collections: 188808\n\nğŸ“¦ Creating compressed archive for download...\n  adding: chroma_db/ (stored 0%)\n  adding: chroma_db/98b16966-98a1-4177-aa1a-7e56bfb1ac5d/ (stored 0%)\n  adding: chroma_db/98b16966-98a1-4177-aa1a-7e56bfb1ac5d/header.bin (deflated 54%)\n  adding: chroma_db/98b16966-98a1-4177-aa1a-7e56bfb1ac5d/index_metadata.pickle","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/1351414654.py:31: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n  guidelines_test_db = Chroma(\n","output_type":"stream"},{"name":"stdout","text":" (deflated 43%)\n  adding: chroma_db/98b16966-98a1-4177-aa1a-7e56bfb1ac5d/link_lists.bin (deflated 64%)\n  adding: chroma_db/98b16966-98a1-4177-aa1a-7e56bfb1ac5d/data_level0.bin (deflated 10%)\n  adding: chroma_db/98b16966-98a1-4177-aa1a-7e56bfb1ac5d/length.bin (deflated 70%)\n  adding: chroma_db/2f13116e-5472-4689-a051-f846525d265a/ (stored 0%)\n  adding: chroma_db/2f13116e-5472-4689-a051-f846525d265a/header.bin (deflated 57%)\n  adding: chroma_db/2f13116e-5472-4689-a051-f846525d265a/index_metadata.pickle (deflated 45%)\n  adding: chroma_db/2f13116e-5472-4689-a051-f846525d265a/link_lists.bin (deflated 76%)\n  adding: chroma_db/2f13116e-5472-4689-a051-f846525d265a/data_level0.bin (deflated 10%)\n  adding: chroma_db/2f13116e-5472-4689-a051-f846525d265a/length.bin (deflated 50%)\n  adding: chroma_db/chroma.sqlite3 (deflated 49%)\nâœ… Archive created successfully: chroma_db.zip\nğŸ“¦ Archive size: 814.07 MB\nğŸ“ Archive location: /kaggle/working/chroma_db.zip\n\nğŸ‰ MEDICAL DATA INGESTION PIPELINE COMPLETED SUCCESSFULLY!\n============================================================\nâœ… Two medical datasets processed and ingested:\n   ğŸ¥ epfl-llm/guidelines â†’ 'medical_guidelines' collection\n   â“ MedRAG/MedQuAD â†’ 'medrag_qna' collection\nğŸ’¾ ChromaDB database location: /kaggle/working/chroma_db\nğŸ“¦ Downloadable archive: /kaggle/working/chroma_db.zip\nğŸ¤– Embedding model: sentence-transformers/all-MiniLM-L6-v2\nâš¡ Processed on: CUDA\n============================================================\nğŸš€ The vector database is ready for deployment in medical RAG systems!\nğŸ“¥ Download the zip file to use in your medical consultation application.\n","output_type":"stream"}],"execution_count":10},{"id":"e2b8d3cb","cell_type":"markdown","source":"## ğŸ¯ Summary and Usage Instructions\n\n### âœ… **What This Notebook Accomplished:**\n\n1. **ğŸ“¦ Environment Setup** - Installed all required packages for Kaggle GPU environment\n2. **âš™ï¸ Configuration** - Set up consistent constants for file paths and model configuration  \n3. **ğŸ¤– GPU-Optimized Embedding** - Initialized Nomic AI embedding model with GPU acceleration\n4. **ğŸ¥ Guidelines Processing** - Processed EPFL guidelines dataset into `medical_guidelines` collection\n5. **â“ Q&A Processing** - Processed MedRAG/MedQuAD dataset into `medrag_qna` collection\n6. **ğŸ“¦ Archive Creation** - Created downloadable `chroma_db.zip` file\n\n### ğŸ—„ï¸ **Output Structure:**\n```\nchroma_db.zip\nâ””â”€â”€ chroma_db/\n    â”œâ”€â”€ medical_guidelines/     # Clinical guidelines collection\n    â””â”€â”€ medrag_qna/            # Medical Q&A collection\n```\n\n### ğŸš€ **How to Use the Generated Database:**\n\n1. **Download** the `chroma_db.zip` file from Kaggle\n2. **Extract** to your medical RAG system directory\n3. **Connect** to collections using:\n   ```python\n   from langchain_community.vectorstores import Chroma\n   \n   # Guidelines collection\n   guidelines_db = Chroma(\n       persist_directory=\"./chroma_db\",\n       collection_name=\"medical_guidelines\"\n   )\n   \n   # Q&A collection  \n   qna_db = Chroma(\n       persist_directory=\"./chroma_db\", \n       collection_name=\"medrag_qna\"\n   )\n   ```\n\n### ğŸ“Š **Performance Optimizations Applied:**\n- âš¡ **GPU Acceleration** - Leveraged Kaggle GPU for embedding generation\n- ğŸ§  **Memory Management** - Cleared variables between processing stages  \n- ğŸ“„ **Optimal Chunking** - 1000 char chunks with 150 char overlap\n- ğŸ—„ï¸ **Separate Collections** - Isolated guidelines and Q&A for specialized retrieval\n\n**The database is now ready for integration into your medical RAG consultation system!**","metadata":{}}]}